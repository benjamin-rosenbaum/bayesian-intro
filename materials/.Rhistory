contrast = posterior_nopool[,"mu[3]"]-posterior_nopool[,"mu[2]"]
plot(density(contrast))
abline(v=0, col="red", lwd=2)
#------------------------------------------------------------------------------
# partial pooling / random effects model
#------------------------------------------------------------------------------
# Until now, we assumed that the different group means are completely independent.
# However, we can assume that the longevity of fruitflies is not completely independent.
# We can assume there is a joint mean and each group mean differs
# from this joint mean with a group-level standard deviation.
# And we can explicitely model that:
# y_i ~ normal(mu[group_i], sigma), i=1,...,n (n observations)
# mu_j ~ normal(mu_total, sigma_total), j=1,...,m (m groups)
# sigma describes within-groups variation of response values,
# sigma_total describes between-groups variation of group means.
# Here, group is treated as a random effect.
# Because some information is shared / pooled across groups,
# this is also called "partial pooling".
# mu_group and sigma_group are also parameters that are estimated from the data.
# Because mu_j's distribution depends on mu_total and sigma_total,
# this is a hierarchical model.
# Like all other parameters, they will be assigned a prior distribution, too.
# For the joint mean mu_total, we can use the same expectation we used for the group means before.
# For the between-groups standard deviation sigma_total,
# it is standard procedure to use half-Chauchy distributions
# (more heavy-tailed than normal distribution)
# The partial pooling model differs from the no pooling model above only in the joint distribution of the mu_j
# and additional parameters mu_total and sigma_total.
stan_code_partpool = '
data {
int n;
int n_group;
real y[n];
int group[n];
}
parameters {
real<lower=0> mu[n_group];
real<lower=0> sigma;
real<lower=0> mu_total;
real<lower=0> sigma_total;
}
model {
// priors
// mu_total ~ normal(0, 100);
sigma_total ~ cauchy(0, 10);
for (j in 1:n_group){
mu[j] ~ normal(mu_total, sigma_total);
}
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal(mu[group[i]], sigma);
}
}
'
stan_model_partpool = stan_model(model_code=stan_code_partpool)
# save(stan_model_partpool, file="stan_code_partpool.RData")
# load("stan_code_partpool.RData")
fit_partpool  = sampling(stan_model_partpool,
data=data,
chains=3,
iter=2000,
warmup=1000,
control=list(adapt_delta=0.9)
)
print(fit_partpool, digits=3, probs=c(0.025, 0.975))
plot(fit_partpool)
# plot(fit_partpool, pars="mu")
# plot(As.mcmc.list(fit_partpool)) # from coda package
#------------------------------------------------------------------------------
# Comparison
#------------------------------------------------------------------------------
# Now we compare the results of the "no pooling" and the "partial pooling" models.
# The means and 95% confidence intervals of the mu_j can directly be extracted in a summary.
summary_nopool = summary(fit_nopool)$summary
summary_nopool
summary_partpool = summary(fit_partpool)$summary
summary_partpool
plot(0, 0, xlim =c(0.5,3.5), ylim = range(df$flipper_length_mm), type = "n",
ylab="bill_length [mm]",
xlab="species")
points(flipper_length_mm ~ jitter(as.numeric(species), factor=0.2), data=df,
col="grey")
points(summary_nopool[1:3, "mean"], pch="+", col="blue", cex=2)
points(summary_nopool[1:3, "2.5%"], pch="-", col="blue", cex=2)
points(summary_nopool[1:3, "97.5%"], pch="-", col="blue", cex=2)
points(summary_partpool[1:3, "mean"], pch="+", col="red", cex=2)
points(summary_partpool[1:3, "2.5%"], pch="-", col="red", cex=2)
points(summary_partpool[1:3, "97.5%"], pch="-", col="red", cex=2)
abline(h=mean(df$flipper_length_mm), col="grey")
abline(h=summary_partpool["mu_total", "mean"], col="red")
legend("topright", legend=c("no pooling","partial pooling"), pch=c("+","+"), col=c("blue","red"), bty="n")
# The difference between "no pooling" and "partial pooling" estimates is that
# extreme values tend to be pulled towards the joint mean by partial pooling (shrinkage).
# Statistical power is borrowed across groups.
# This can be helpful if some groups have a low number of observations.
# In addition to both models, there is also "complete pooling".
# Here, all information across groups would be pooled:
# y_i ~ normal(mu, sigma)
# There is no effect of group included, i.e. all groups share the same mean.
# "partial pooling" is a compromise between "no pooling" and "complete pooling".
rm(list=ls())
library(rstan)
library(coda)
library(BayesianTools)
# setwd("~/Desktop/teaching Bayes")
# remotes::install_github("allisonhorst/palmerpenguins")
library(palmerpenguins)
set.seed(123) # initiate random number generator for reproducability
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
# In the previous model, we just fitted mean values to groups
# and there was still some unexplained variation.
# Here, we will add a continuous predictor (covariate).
# Specifically, for the same dataset of fruitfly longevity and sexual activity,
# we add the covariate individual body size.
# Lifespan is assumed to be positively correlated with body size.
#------------------------------------------------------------------------------
# load data
#------------------------------------------------------------------------------
df = penguins
head(df)
str(df)
df = df[complete.cases(df), ]
plot(df$body_mass_g, df$bill_length_mm, col=as.factor(df$species))
# Note that the range of x-values (body size) is far away from zero.
# This makes intercepts on regression hard to interpret.
# (what is the longevity of a fruitfly with zero body size?)
# We will use normalized (zscore) predictor values instead.
df$body_mass_g_norm = as.numeric(scale(df$body_mass_g))
df$group = as.integer(df$species)
plot(df$body_mass_g_norm, df$bill_length_mm, col=as.factor(df$group))
# as.integer(<factor>) codes groups in alphabetical order
table(df$group, df$species)
par(mfrow=c(3,1))
for (i in 1:3){
df.sub=subset(df, df$group==i)
plot(df.sub$body_mass_g_norm,  df.sub$bill_length_mm,
xlim=range(df$body_mass_g_norm),
ylim=range(df$bill_length_mm),
main=levels(df$species)[i]
)
}
data = list(y = df$bill_length_mm,
x = df$body_mass_g_norm,
group = df$group,
n = nrow(df),
n_group = 3)
#------------------------------------------------------------------------------
# partial pooling model
#------------------------------------------------------------------------------
# We will fit linear regression lines to each group ("CompanionNumber) as follows:
# y_i ~ normal(a[group_i]+b*x_i, sigma) i=1,...,n (n observations)
# a_j ~ normal(mu_a, sigma_a) j=1,...,m (m groups)
# Here, a_j is a group-level intercept, which are allowed to vary (partial pooling).
# (Analogue to mu_j in the previous example)
# But we assume identical slope b for all groups (complete pooling).
# So this is a random intercepts linear regression,
# lm-formulation would be "y ~ (1|group) + x".
# The approach is also similar to frequentist ANCOVA.
# The Stan code differs from the previous model by adding
# the covariate x and parameter slope b
# (and renaming mu to a, that's it!)
stan_code_partial = '
data {
int n;
int n_group;
real y[n];
real x[n];
int group[n];
}
parameters {
real a[n_group];
real b;
real<lower=0> sigma;
real mu_a;
real<lower=0> sigma_a;
}
model {
// priors
mu_a ~ normal(0,100);
sigma_a ~ cauchy(0,10);
for (j in 1:n_group){
a[j] ~ normal(mu_a,sigma_a);
}
b ~ normal(0,100);
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal(a[group[i]]+b*x[i], sigma);
}
}
'
stan_model_partial = stan_model(model_code=stan_code_partial)
# save(stan_model_partial, file="stan_code_partial.RData")
# load("stan_code_partial.RData")
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=2000,
warmup=1000
)
print(fit_partial, digits=3, probs=c(0.025, 0.975))
plot(fit_partial)
# plot(fit_partial, pars="a")
# plot(As.mcmc.list(fit_partial)) # from coda package
posterior=as.matrix(fit_partial)
# As before, we can look at the individual differences
# of intercepts between groups ("contrasts").
# E.g., the posterior distribution of a4-a5
contrast = posterior[,"a[2]"]-posterior[,"a[3]"]
par(mfrow=c(1,1))
plot(density(contrast))
abline(v=0, col="red", lwd=2)
#------------------------------------------------------------------------------
# predictions / credible intervals
#------------------------------------------------------------------------------
# Again, we can generate predictions and compute credible intervals (for the deterministic part of the model)
# and prediction intervals (for the data, including statistical part of the model).
# (Here: 90% credible intervals)
x.pred = seq(from=min(df$body_mass_g_norm), to=max(df$body_mass_g_norm), by=0.1)
par(mfrow=c(3,1))
for (i in 1:3){
df.sub=subset(df, df$group==i)
plot(df.sub$body_mass_g_norm,  df.sub$bill_length_mm,
xlim=range(df$body_mass_g_norm),
ylim=range(df$bill_length_mm),
main=levels(df$species)[i]
)
y.cred = matrix(0, nrow=nrow(posterior), ncol=length(x.pred))
for(j in 1:nrow(posterior)){
# column i in posterior corresponds to a_i, alternatively reference by name:
# posterior[j,paste0("a[",i,"]")]
y.cred[j, ] = posterior[j,i] + posterior[j,"b"]*x.pred
}
y.cred.mean = apply(y.cred, 2, function(x) mean(x))
lines(x.pred, y.cred.mean, col="red", lwd=2)
y.cred.q05 = apply(y.cred, 2, function(x) quantile(x, probs=0.05))
lines(x.pred, y.cred.q05, col="red", lwd=2, lty=2)
y.cred.q95 = apply(y.cred, 2, function(x) quantile(x, probs=0.95))
lines(x.pred, y.cred.q95, col="red", lwd=2, lty=2)
}
rm(list=ls())
library(rstan)
library(coda)
library(BayesianTools)
# setwd("~/Desktop/teaching Bayes")
# remotes::install_github("allisonhorst/palmerpenguins")
library(palmerpenguins)
set.seed(123) # initiate random number generator for reproducability
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
# Extend the last model by including random slopes, too!
#------------------------------------------------------------------------------
# load data
#------------------------------------------------------------------------------
df = penguins
head(df)
str(df)
df = df[complete.cases(df), ]
df$body_mass_g_norm = as.numeric(scale(df$body_mass_g))
df$group = as.integer(df$species)
data = list(y = df$bill_length_mm,
x = df$body_mass_g_norm,
group = df$group,
n = nrow(df),
n_group = 3)
par(mfrow=c(3,1))
for (i in 1:3){
df.sub=subset(df, df$group==i)
plot(df.sub$body_mass_g_norm,  df.sub$bill_length_mm,
xlim=range(df$body_mass_g_norm),
ylim=range(df$bill_length_mm),
main=levels(df$species)[i]
)
}
#------------------------------------------------------------------------------
# partial pooling model
#------------------------------------------------------------------------------
# We will fit linear regression lines to each group ("CompanionNumber) as follows:
# y_i ~ normal(a[group_i]+b[group_i]*x_i, sigma) i=1,...,n (n observations)
# a_j ~ normal(mu_a, sigma_a) j=1,...,m (m groups)
# b_j ~ normal(mu_b, sigma_b) j=1,...,m (m groups)
# Here, a_j and b_j are group-level intercepts and slopes,
# which are allowed to vary (partial pooling).
# Both have their own means and standard deviations, which are also free parameters to be estimated.
# So this is a random intercepts and slopes linear regression, lm-formulation would be `y ~ x + (x|group)`,
# which is short for `y ~ 1+x + (1+x|group)`.
stan_code_partial = '
data {
int n;
int n_group;
real y[n];
real x[n];
int group[n];
}
parameters {
real a[n_group];
real b[n_group];
real<lower=0> sigma;
real mu_a;
real<lower=0> sigma_a;
real mu_b;
real<lower=0> sigma_b;
}
model {
// priors
mu_a ~ normal(0,100);
mu_b ~ normal(0,10);
sigma_a ~ cauchy(0,1);
sigma_b ~ cauchy(0,1);
for (j in 1:n_group){
a[j] ~ normal(mu_a,sigma_a);
b[j] ~ normal(mu_b,sigma_b);
}
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal( a[ group[i] ] + b[ group[i] ] *x[i] , sigma);
}
}
'
stan_model_partial = stan_model(model_code=stan_code_partial)
# save(stan_model_partial, file="stan_code_partial.RData")
# load("stan_code_partial.RData")
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=2000,
warmup=1000
)
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=5000,
warmup=2000,
control=list(adapt_delta=0.99)
)
print(fit_partial, digits=3, probs=c(0.025, 0.975))
plot(fit_partial)
# plot(fit_partial, pars="a")
plot(As.mcmc.list(fit_partial)) # from coda package
# pairs(fit_partial, pars=c("b","mu_b","sigma_b"))
posterior=as.matrix(fit_partial)
#------------------------------------------------------------------------------
# predictions / credible intervals
#------------------------------------------------------------------------------
# Again, we can generate predictions and compute credible intervals (for the deterministic part of the model)
# (Here: 90% credible intervals)
x.pred = seq(from=min(df$body_mass_g_norm), to=max(df$body_mass_g_norm), by=0.1)
par(mfrow=c(3,1))
for (i in 1:3){
df.sub=subset(df, df$group==i)
plot(df.sub$body_mass_g_norm,  df.sub$bill_length_mm,
xlim=range(df$body_mass_g_norm),
ylim=range(df$bill_length_mm),
main=levels(df$species)[i]
)
y.cred = matrix(0, nrow=nrow(posterior), ncol=length(x.pred))
for(j in 1:nrow(posterior)){
y.cred[j, ] = posterior[j,paste0("a[",i,"]")] + posterior[j,paste0("b[",i,"]")]*x.pred
}
y.cred.mean = apply(y.cred, 2, function(x) mean(x))
lines(x.pred, y.cred.mean, col="red", lwd=2)
y.cred.q05 = apply(y.cred, 2, function(x) quantile(x, probs=0.05))
lines(x.pred, y.cred.q05, col="red", lwd=2, lty=2)
y.cred.q95 = apply(y.cred, 2, function(x) quantile(x, probs=0.95))
lines(x.pred, y.cred.q95, col="red", lwd=2, lty=2)
}
#------------------------------------------------------------------------------
# Further reading
#------------------------------------------------------------------------------
# read about complete pooling, partial pooling, no pooling and shrinkage:
# https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/
rm(list=ls())
library(rstan)
library(coda)
library(BayesianTools)
# setwd("~/Desktop/teaching Bayes")
# remotes::install_github("allisonhorst/palmerpenguins")
library(palmerpenguins)
set.seed(123) # initiate random number generator for reproducability
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
# Extend the last model by including random slopes, too!
#------------------------------------------------------------------------------
# load data
#------------------------------------------------------------------------------
df = penguins
head(df)
str(df)
df = df[complete.cases(df), ]
df$body_mass_g_norm = as.numeric(scale(df$body_mass_g))
df$group = as.integer(df$species)
data = list(y = df$bill_length_mm,
x = df$body_mass_g_norm,
group = df$group,
n = nrow(df),
n_group = 3)
par(mfrow=c(3,1))
for (i in 1:3){
df.sub=subset(df, df$group==i)
plot(df.sub$body_mass_g_norm,  df.sub$bill_length_mm,
xlim=range(df$body_mass_g_norm),
ylim=range(df$bill_length_mm),
main=levels(df$species)[i]
)
}
#------------------------------------------------------------------------------
# partial pooling model
#------------------------------------------------------------------------------
# We will fit linear regression lines to each group ("CompanionNumber) as follows:
# y_i ~ normal(a[group_i]+b[group_i]*x_i, sigma) i=1,...,n (n observations)
# a_j ~ normal(mu_a, sigma_a) j=1,...,m (m groups)
# b_j ~ normal(mu_b, sigma_b) j=1,...,m (m groups)
# Here, a_j and b_j are group-level intercepts and slopes,
# which are allowed to vary (partial pooling).
# Both have their own means and standard deviations, which are also free parameters to be estimated.
# So this is a random intercepts and slopes linear regression, lm-formulation would be `y ~ x + (x|group)`,
# which is short for `y ~ 1+x + (1+x|group)`.
stan_code_partial = '
data {
int n;
int n_group;
real y[n];
real x[n];
int group[n];
}
parameters {
real a[n_group];
real b[n_group];
real<lower=0> sigma;
real mu_a;
real<lower=0> sigma_a;
real mu_b;
real<lower=0> sigma_b;
}
model {
// priors
mu_a ~ normal(0,100);
mu_b ~ normal(0,10);
sigma_a ~ cauchy(0,1);
sigma_b ~ cauchy(0,1);
for (j in 1:n_group){
a[j] ~ normal(mu_a,sigma_a);
b[j] ~ normal(mu_b,sigma_b);
}
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal( a[ group[i] ] + b[ group[i] ] *x[i] , sigma);
}
}
'
stan_model_partial = stan_model(model_code=stan_code_partial)
# save(stan_model_partial, file="stan_code_partial.RData")
# load("stan_code_partial.RData")
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=2000,
warmup=1000
)
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=5000,
warmup=2000,
control=list(adapt_delta=0.99)
)
print(fit_partial, digits=3, probs=c(0.025, 0.975))
plot(fit_partial)
# plot(fit_partial, pars="a")
plot(As.mcmc.list(fit_partial)) # from coda package
# pairs(fit_partial, pars=c("b","mu_b","sigma_b"))
posterior=as.matrix(fit_partial)
#------------------------------------------------------------------------------
# predictions / credible intervals
#------------------------------------------------------------------------------
# Again, we can generate predictions and compute credible intervals (for the deterministic part of the model)
# (Here: 90% credible intervals)
x.pred = seq(from=min(df$body_mass_g_norm), to=max(df$body_mass_g_norm), by=0.1)
par(mfrow=c(3,1))
for (i in 1:3){
df.sub=subset(df, df$group==i)
plot(df.sub$body_mass_g_norm,  df.sub$bill_length_mm,
xlim=range(df$body_mass_g_norm),
ylim=range(df$bill_length_mm),
main=levels(df$species)[i]
)
y.cred = matrix(0, nrow=nrow(posterior), ncol=length(x.pred))
for(j in 1:nrow(posterior)){
y.cred[j, ] = posterior[j,paste0("a[",i,"]")] + posterior[j,paste0("b[",i,"]")]*x.pred
}
y.cred.mean = apply(y.cred, 2, function(x) mean(x))
lines(x.pred, y.cred.mean, col="red", lwd=2)
y.cred.q05 = apply(y.cred, 2, function(x) quantile(x, probs=0.05))
lines(x.pred, y.cred.q05, col="red", lwd=2, lty=2)
y.cred.q95 = apply(y.cred, 2, function(x) quantile(x, probs=0.95))
lines(x.pred, y.cred.q95, col="red", lwd=2, lty=2)
}
#------------------------------------------------------------------------------
# Further reading
#------------------------------------------------------------------------------
# read about complete pooling, partial pooling, no pooling and shrinkage:
# https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/
pairs(fit_partial, pars=c("b","mu_b","sigma_b"))
# plot(fit_partial, pars="a")
plot(fit_partial, pars="b")
# plot(fit_partial, pars="a")
plot(fit_partial, pars="a")
plot(fit_partial, pars="b")
