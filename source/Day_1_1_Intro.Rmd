---
title: "1.1: Introduction"
author: "Benjamin Rosenbaum"
date: "April 1, 2019"
output: ioslides_presentation
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Contents {.smaller}

### **Day 1**
- statistical models & the likelihood
- Bayesian principles
- Stan: a probabilistic programming language
- my first Stan model

### **Day 2**
- priors 
- interpreting results & working with the posterior distribution
- more classical models

### **Day 3**
- hierarchical models
- resources
- summary 


## Terminology

- $p(y)$ probability density function
- $P(A)=P(y\in A)=\int_A p(y)dy$ probability of the event $A$:   
    e.g. $P(y>0)=P(y\in[0,\infty])=\int_0^\infty p(y)dy$ 
- $P(A|B)$ conditional probability:  
    probability of $A$, given $B$ already occured
- $y\sim\text{normal}(\mu,\sigma)$:   
    $y$ is a normally distributed random variable   
    with mean $\mu$ and standard deviation $\sigma$
- know your distribution!
    https://ben18785.shinyapps.io/distribution-zoo/

## Goals

- think in terms of: model, data, parameters
- building blocks of statistics 
- not be intimidated by Bayesian stats
- understand basic examples
- interpret model output
- **build your own models**



## Statistical modeling

- data are described by underlying statistical model
- model: deterministic and stochastic part
- **deterministic part**: mechanistic, phenomenological
- **stochastic part**: connects mechanistic part to the data
- inference: what can the data tell me about my model? 




## A basic example: linear regression

$y_i \sim \text{normal}(\mu_i,\sigma)\quad i=1,...,n$

$\mu_i = a+b\cdot x_i$


```{r linreg, fig.height=4, fig.width=6, fig.align="left"}
set.seed(123)
n = 15
x = runif(n, 0, 1)
y = rnorm(n, 1+2*x, 0.4)
par(mfrow=c(1,1), mar=c(4,4,1,2), oma=c(0,0,0,0))
plot(x,y, ylim=c(1,4), type="n")
abline(1,2, lwd=2, col="blue")
for(i in 1:n){
  lines(c(x[i],x[i]), c(y[i], 1+2*x[i]), lwd=2, col="red", lty=3)
}
points(x,y, pch=16, cex=1)
```

## A basic example: linear regression

$y_i \sim \text{normal}(\mu_i,\sigma)\quad i=1,...,n$

$\mu_i = a+b\cdot x_i$

- data: $x_i, y_i,\quad i=1,...,n$
- parameters: $a,b,\sigma$
- deterministic part: $\mu_i = a+b\cdot x_i$
- stochastic part: $y_i \sim \text{normal}(\mu_i,\sigma)$  
      or written as residuals: $\epsilon_i =  y_i -\mu_i \sim \text{normal}(0,\sigma)$


## A basic example: linear regression

$y_i \sim \text{normal}(\mu_i,\sigma)\quad i=1,...,n$

$\mu_i = a+b\cdot x_i$

### do the data have to be normally distributed?

- common misconception, even in some textbooks!
- each datapoint $y_i$ is normally distributed with a "shifting" mean $\mu_i$ ($y_i$ are **not** iid)
- so the data $y_1,\dots,y_n$ are **not** normally distributed with a joint mean $\mu$ 
- but the residuals $\epsilon_i = y_i-\mu_i\sim \text{normal}(0,\sigma)$ are iid with zero mean!  
- generally, there are no assumptions on the raw data, but on the residuals!



## Even simpler: estimating the mean

Yes, this is already a statistical model!

$y_i \sim \text{normal}(\mu,\sigma)\quad i=1,...,n$

```{r estmean, fig.height=4, fig.width=6, fig.align="left"}
set.seed(123)
n = 40
y = rnorm(n, 0, 1)
par(mfrow=c(1,1), mar=c(4,4,1,2), oma=c(0,0,0,0))
hist(y)
# plot(y, type="p")
points(y,rep(0,n))

x.plot=seq(-4,4,by=0.01)

lines(x.plot, 21*dnorm(x.plot), lwd=2, col="red")

```


## The likelihood function


- a single datapoint $y_i$
- probability density function $p()$
- here: normal distribution $y_i\sim \text{normal}(\mu,\sigma)$
- $p(y_i | \mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-\mu)^2}{2\sigma^2}  }$ 

```{r test, fig.height=3.2, fig.width=6, fig.align="left"}
par(mfrow=c(1,1), mar=c(4,4,1,2), oma=c(0,0,0,0))
curve(dnorm, from=-4, to=4, ylab="p(y)", xlab="y", lwd=2, col="blue")
```


## Maximum likelihood principle 

Find parameters $\theta=(\mu,\sigma)$ that maximize likelihood of all datapoints $y_1,...,y_n$ simultaneously!

Or: for which parameters $\theta$ is the given data $y$ most likely?

$\max p(y_1,y_2,...,y_n|\theta)$

$\max p(y_1|\theta)\cdot p(y_2|\theta) \cdot ... \cdot p(y_n|\theta)$

$\max \log\{p(y_1|\theta)\cdot p(y_2|\theta) \cdot ... \cdot p(y_n|\theta)\}$

$\max \{ \log p(y_1|\theta) + \log p(y_2|\theta) + ... + \log p(y_n|\theta)\}$


