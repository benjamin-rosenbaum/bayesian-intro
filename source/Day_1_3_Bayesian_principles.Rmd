---
title: "1.3: Bayesian principles"
author: "Benjamin Rosenbaum"
date: "October 21, 2019"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## What's "wrong" with max. likelihood?

- $p(y|\theta)$ is a probability function for the data $y$ given the parameters $\theta$ 
- Inference on parameters, e.g. $P(\theta>0)$, not straightforward possible using $p(y|\theta)$   
- But the data is already observed, we don't want to calculate probabilities for $y$
- What we actually want is probability of parameters $\theta$ given the observed data $y$
- Or: what does the data tell us about our model?
- We need $p(\theta|y)$ !


## What's "wrong" with P-values?

- $p(y|\theta)$ is a probability function for the data $y$ given the parameters $\theta$
- calculation of $P(\theta>0)$ or $P(\theta=0)$ not possible
- so what does the P-value mean?

workaround: 

- transform data to a single value $T(y)$ with teststatistic $T$
- assume parameter was actually zero ($H_0:\theta=0$)
- calculate probability for any data being different from observed data $P(T(\tilde y)>T(y)|\theta=0)$ (that's the P-value)
- if this is small, reject null-hypothesis $H_0$

Does this procedure sound weird? Let's check out $p(\theta|y)$ now!


## What are we interested in? 

### Example: linear regression
$y_i \sim \text{normal}(\mu_i,\sigma)\quad i=1,...,n$  
$\mu_i = a+b\cdot x_i$

- interested in effect, i.e. slope $b$
- what do the data tell me about the slope? (evidence)
- probability distribution $p(b|y)$ **given the data**
- mean estimate, confidence intervals, $P(b>0)$, ...
- no P-values needed
- **direct inference** instead


## Bayes rule

For observed data $y$ and model parameters $\theta$

$$p(\theta|y)=\frac{p(\theta)\cdot p(y|\theta)}{p(y)}$$

- $p(\theta|y)$ posterior distribution
- $p(\theta)$ prior distribution
- $p(y|\theta)$ likelihood function
- $p(y) = \int_\theta p(\theta) p(y|\theta) d\theta$ just a normalization constant, independent of actual $\theta$

## Bayes rule

posterior is proportional to ("$\varpropto$") prior times likelihood!

$$p(\theta|y)\varpropto p(\theta)\cdot p(y|\theta)$$



```{r posterior, fig.height=4.5, fig.width=6, fig.align="left"}
x=seq(-10,10, by=0.01)
prior=dnorm(x, mean=0, sd=4)
likelihood=dnorm(x, mean=2, sd=1.5)
par(mfrow=c(1,1), mar=c(4,4,1,1), oma=c(1,0,0,0))
plot(x,prior, type="l", col="blue", ylim=c(0,0.3), lwd=2, lty=2, xlab=expression(theta), ylab=("density"))
lines(x,likelihood, col="red", lwd=2, lty=2)
lines(x,prior*likelihood/sum(prior*likelihood)*100, col="purple", lwd=2)
legend("topleft",legend=c("prior", "likelihood", "posterior"), lty=c(1,1,1), lwd=c(2,2,2), col=c("blue","red","purple"))
```

## Bayes rule

posterior is proportional to ("$\varpropto$") prior times likelihood!

$$p(\theta|y)\varpropto p(\theta)\cdot p(y|\theta)$$

- prior: a-priori information of the model parameters
- likelihood: information of the data
- posterior: updated information of the model parameters given the data

## Excursion: some history 

<!-- ![Thomas Bayes](pics/Thomas_Bayes_small.png) ![Pierre Laplace](pics/Pierre-Simon_Laplace_small.png) ![Ronald Fisher](pics/R_A_Fischer_small.png) -->

<!-- - **1700s** Bayes, Laplace -->
<!-- - **early 1900s** Fisher: frequentism, maximum likelihood -->
<!-- - **mid -- late 1900s** MCMC algorithms (Metropolis-Hastings, Gibbs sampling, Hamiltonian MC) -->
<!-- - **2000s** computational methods for MCMC   -->
<!--     (BUGS, JAGS, Stan, ...) -->

- **1700s.** Bayes' theorem, Laplace formalized it 
- **early 1800s.** GauÃŸ: least squares, regression
- **late 1800s to early 1900s.** modern statistics  
Galton, Pearson, Fisher, ... :  
design of experiments, hypothesis testing, max likelihood  
frequentism usurped Bayesian stats  
- **mid to late 1900s.** MCMC algorithms
- **2000s.** computational tools for MCMC  
(BUGS, JAGS, Stan, ...)

## Posterior: local or global?

### local approach

- find $\theta$ that maximizes $P(\theta|y)$
- analogue to maximum likelihood
- maximum a posteriori estimate (MAP)
- **mode** of posterior distribution

### global approach

- explore $P(\theta|y)$ in whole parameter space
- compute all statistics: mean, sd, CIs, $P(\theta>0)$, $P(a<\theta<b)$, ...
- **but how to evaluate $P(\theta|y)$ globally?**

## The curse of dimensionality | Naive approach e.g.: sampling with grids

statistical model with 1 parameter:  
evaluate $P(\theta|y)$ "globally" in $m=10$ locations  

<!-- ![](pics/grid_example_1d.png) --> 

```{r curse 1d, fig.height=4.5, fig.width=6, fig.align="left"}
x=seq(-3,3, length.out=10)
p=dnorm(x, mean=0, sd=1)
par(mfrow=c(1,1), mar=rep(0,4), oma=c(4,4,0,2))
plot(x,p, type="l", ylim=c(0,max(p)),
     xlab="", ylab="",
     xaxt="n",
     yaxt="n",
     lwd=2,
     lty=2
     )
mtext("theta", side=1, line=1)
mtext("density", side=2, line=1)
for (i in 1:length(x)){
  lines(c(x[i],x[i]),c(0,p[i]), lwd=2, lty=1)
}
```

## The curse of dimensionality | Naive approach e.g.: sampling with grids

statistical model with 2 parameters:  
evaluate $P(\theta|y)$ "globally" in $m\cdot m=10\cdot10=100$ locations  

```{r curse 2d, fig.height=4.5, fig.width=6, fig.align="left"}
library("plot3D")
x=seq(-2,2, length.out=10)
y=x
xy=expand.grid(x=x,y=y)
p = with(xy, exp(-x^2-y^2))
par(mfrow=c(1,1), mar=rep(0,4), oma=c(4,4,0,2))
persp(x=x, 
      y=y, 
      z=matrix(p,length(x),length(x)),
      xlab=expression(theta1),
      ylab=expression(theta2),
      zlab="density",
      theta = -30, 
      phi = 30,
      lwd=2)
```


## The curse of dimensionality | Naive approach e.g.: sampling with grids 

statistical model with $n$ parameters:  
evaluate $P(\theta|y)$ "globally" in $m^n$ locations  

- models with e.g. $n=20$ parameters out of reach! 
- (and $m=10$ evaluations per dimension is still quite coarse)
- CPU time = millions of years
- grids perform many evaluations where probability density is low ("empty space")

**We need a more efficient sampling strategy!**


## Markov Chain Monte Carlo (MCMC) 

**Monte Carlo:** each iteration/sample is drawn randomly  
    (with some rule)

**Markov:** each new sample depends only on the last one,
iterative routine

**Chain:** list of samples

- this rule is specified such that  
    - many samples are drawn where $P(\theta|y)$ is high and  
    - few samples are drawn where $P(\theta|y)$ is low

- **chain represents a random draw from the posterior distribution!!**

## Markov Chain Monte Carlo (MCMC)

let's see some example. 

follow the link and choose from different MCMC algorithms and several targets (i.e. posteriors to be explored)

https://chi-feng.github.io/mcmc-demo/app.html#RandomWalkMH,banana 

- These are all 2d examples. 
- Compare the 1d marginal distributions on the bottom and on the left with the complete 2d representation. 
- Would a local approach (maximum a posteriori or maximum likelihood) make sense for all examples?

## Markov Chain Monte Carlo (MCMC)

![](pics/MCMC_example_small.png)

## Why go Bayesian?

Philosophical answer:

- **Frequentism** assumes true and fixed underlying parameters.  
- Data are just a sample of the "true" statistical model.

- **Bayesian** stats embraces uncertainty.
- Observed data are given, model parameters uncertain.


## Why go Bayesian?

Practical answer:

- Fit complex models with lots of parameters.
- Full transparency and control over your model and parameters.
- Probability descibes uncertainty.  
    Quantify uncertainty **exactly** (sd, CIs).
- Output is intuitive:   
    Direct inference instead of null hypothesis testing,   
    e.g. $P(\theta>0\ |\ y)$.  
    What does data tell me about my model?
- Include prior belief / information.


